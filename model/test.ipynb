{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b0c5cbb1-8ce8-4922-9299-1f96bc530cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "import sys\n",
    "from activations import ACT2FN\n",
    "from pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n",
    "\n",
    "from transformers import GPT2Config, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import OrderedDict\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b8322-24d4-4e50-8fc2-51946c574d29",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e85abf51-1c69-447f-8be2-f160a324b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config, is_cross_attention=False, layer_idx=None):\n",
    "        super().__init__()\n",
    "\n",
    "        max_positions = config.max_position_embeddings\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                1, 1, max_positions, max_positions\n",
    "            ),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.split_size = self.embed_dim\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.scale_attn_weights = config.scale_attn_weights\n",
    "        self.is_cross_attention = is_cross_attention\n",
    "\n",
    "        # Layer-wise attention scaling, reordering, and upcasting\n",
    "        self.scale_attn_by_inverse_layer_idx = config.scale_attn_by_inverse_layer_idx\n",
    "        self.layer_idx = layer_idx\n",
    "        self.reorder_and_upcast_attn = config.reorder_and_upcast_attn\n",
    "\n",
    "        if self.is_cross_attention:\n",
    "            self.c_attn = Conv1D(2 * self.embed_dim, self.embed_dim)\n",
    "            self.q_attn = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim)\n",
    "        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n",
    "        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n",
    "\n",
    "        # Prune conv1d layers\n",
    "        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n",
    "        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n",
    "\n",
    "        # Update hyper params\n",
    "        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n",
    "        self.num_heads = self.num_heads - len(heads)\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "        if self.scale_attn_weights:\n",
    "            attn_weights = attn_weights / torch.full(\n",
    "                [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
    "            )\n",
    "\n",
    "        # Layer-wise attention scaling\n",
    "        if self.scale_attn_by_inverse_layer_idx:\n",
    "            attn_weights = attn_weights / float(self.layer_idx + 1)\n",
    "\n",
    "        if not self.is_cross_attention:\n",
    "            # if only \"normal\" attention layer implements causal mask\n",
    "            query_length, key_length = query.size(-2), key.size(-2)\n",
    "            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "            mask_value = torch.finfo(attn_weights.dtype).min\n",
    "            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "            mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
    "        attn_weights = attn_weights.type(value.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n",
    "        bsz, num_heads, q_seq_len, dk = query.size()\n",
    "        _, _, k_seq_len, _ = key.size()\n",
    "\n",
    "        # Preallocate attn_weights for `baddbmm`\n",
    "        attn_weights = torch.empty(bsz * num_heads, q_seq_len, k_seq_len, dtype=torch.float32, device=query.device)\n",
    "\n",
    "        # Compute Scale Factor\n",
    "        scale_factor = 1.0\n",
    "        if self.scale_attn_weights:\n",
    "            scale_factor /= float(value.size(-1)) ** 0.5\n",
    "\n",
    "        if self.scale_attn_by_inverse_layer_idx:\n",
    "            scale_factor /= float(self.layer_idx + 1)\n",
    "\n",
    "        # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n",
    "        with autocast(enabled=False):\n",
    "            q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n",
    "            attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n",
    "            attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)\n",
    "\n",
    "        if not self.is_cross_attention:\n",
    "            # if only \"normal\" attention layer implements causal mask\n",
    "            query_length, key_length = query.size(-2), key.size(-2)\n",
    "            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "            mask_value = torch.finfo(attn_weights.dtype).min\n",
    "            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
    "            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\n",
    "        if attn_weights.dtype != torch.float32:\n",
    "            raise RuntimeError(\"Error with upcasting, attn_weights does not have dtype torch.float32\")\n",
    "        attn_weights = attn_weights.type(value.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    def _split_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "        if encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"q_attn\"):\n",
    "                raise ValueError(\n",
    "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
    "                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
    "                )\n",
    "\n",
    "            query = self.q_attn(hidden_states)\n",
    "            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        if self.reorder_and_upcast_attn:\n",
    "            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
    "        else:\n",
    "            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs  # a, present, (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735cd24c-85b8-4b59-8f4a-fa4eb2ac5d3c",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a5052e26-ae5c-4d5b-8bb5-92afbeb2dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self, intermediate_size, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        self.c_fc = Conv1D(intermediate_size, embed_dim)\n",
    "        self.c_proj = Conv1D(embed_dim, intermediate_size)\n",
    "        self.act = ACT2FN[config.activation_function]\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8213d5-29d5-4235-863a-07119ba18529",
   "metadata": {},
   "source": [
    "#### GPT2 Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1e72248a-d32a-49c9-8f8d-82e642a00b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__()\n",
    "        hidden_size = config.hidden_size\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.attn = GPT2Attention(config, layer_idx=layer_idx)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        if config.add_cross_attention:\n",
    "            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "            self.crossattention = GPT2Attention(config, is_cross_attention=True, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = GPT2MLP(inner_dim, config)\n",
    "\n",
    "        # Add: Update memory\n",
    "        self.crossattention_u = GPT2Attention(config, is_cross_attention=True, layer_idx=layer_idx)\n",
    "        self.mlp_u = GPT2MLP(inner_dim, config)\n",
    "        self.ln_f_u = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        memory_state = None,\n",
    "        memory_state_u = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        \n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        \n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        # Read memory\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            memory_hidden_states = torch.cat([memory_state, hidden_states], dim=1) # Add\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=memory_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "\n",
    "        # Update memory\n",
    "        residual = memory_state_u\n",
    "        hidden_memory_state = self.ln_cross_attn(memory_state_u)\n",
    "        memory_hidden_states = torch.cat([memory_state, hidden_states], dim=1) # Add\n",
    "        cross_attn_outputs_u = self.crossattention_u(\n",
    "                hidden_memory_state,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=memory_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "        attn_output_u = cross_attn_outputs_u[0]\n",
    "        # residual connection\n",
    "        memory_state_u = residual + attn_output_u\n",
    "\n",
    "        residual = memory_state_u\n",
    "        memory_state_u = self.ln_2(memory_state_u)\n",
    "        feed_forward_memory_states = self.mlp_u(memory_state_u)\n",
    "        # residual connection\n",
    "        memory_state_u = residual + feed_forward_memory_states\n",
    "        memory_state_u = self.ln_f_u(memory_state_u)\n",
    "\n",
    "        \n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return (memory_state_u, outputs)  # hidden_states, present, (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9cc67-3a53-4028-84f2-670c0983d5a1",
   "metadata": {},
   "source": [
    "#### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "501da030-edae-4d2f-81f5-e15237b2d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        self.dtype = torch.float16\n",
    "        \n",
    "    def get_input_embeddings(self):\n",
    "        return self.wte\n",
    "\n",
    "    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Invert an attention mask (e.g., switches 0. and 1.).\n",
    "        \n",
    "        Args:\n",
    "            encoder_attention_mask (`torch.Tensor`): An attention mask.\n",
    "        \n",
    "        Returns:\n",
    "            `torch.Tensor`: The inverted attention mask.\n",
    "        \"\"\"\n",
    "        if encoder_attention_mask.dim() == 3:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
    "        if encoder_attention_mask.dim() == 2:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
    "        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n",
    "        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow\n",
    "        # /transformer/transformer_layers.py#L270\n",
    "        # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n",
    "        # encoder_extended_attention_mask.transpose(-1, -2))\n",
    "        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * torch.finfo(self.dtype).min\n",
    "        \n",
    "        return encoder_extended_attention_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        memory_state = None,\n",
    "        memory_state_u = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
    "\n",
    "        \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "    \n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()  # batch_size * seq_length \n",
    "            input_ids = input_ids.view(-1, input_shape[-1]) # batch_size * seq_length \n",
    "            batch_size = input_ids.shape[0] # batch*size\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1] # batch_size * seq_length \n",
    "            batch_size = inputs_embeds.shape[0] # batch*size\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "    \n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "    \n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "    \n",
    "        if past_key_values is None:\n",
    "            past_length = 0\n",
    "            past_key_values = tuple([None] * len(self.h))\n",
    "        else:\n",
    "            past_length = past_key_values[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device) \n",
    "            position_ids = position_ids.unsqueeze(0) # 1 * sequence_length\n",
    "    \n",
    "        # GPT2Attention mask.\n",
    "        if attention_mask is not None:\n",
    "            if batch_size <= 0:\n",
    "                raise ValueError(\"batch_size has to be defined and > 0\")\n",
    "            attention_mask = attention_mask.view(batch_size, -1)\n",
    "            # We create a 3D attention mask from a 2D tensor mask.\n",
    "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "            # this attention mask is more simple than the triangular masking of causal attention\n",
    "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "            attention_mask = attention_mask[:, None, None, :] # [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "    \n",
    "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "            # masked positions, this operation will create a tensor which is 0.0 for\n",
    "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
    "            # Since we are adding it to the raw scores before the softmax, this is\n",
    "            # effectively the same as removing these entirely.\n",
    "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
    "    \n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask) # Should set encoder_attention_mask\n",
    "        else:\n",
    "            encoder_attention_mask = None\n",
    "    \n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # # head_mask has shape n_layer x batch x n_heads x N x N\n",
    "        # head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
    "    \n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "    \n",
    "        if token_type_ids is not None:\n",
    "            token_type_embeds = self.wte(token_type_ids)\n",
    "            hidden_states = hidden_states + token_type_embeds\n",
    "    \n",
    "        hidden_states = self.drop(hidden_states)\n",
    "    \n",
    "        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),) # -1 *batch_size * seq_length * num_embed\n",
    "    \n",
    "    \n",
    "        presents = () if use_cache else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            memory_state_u, outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past=layer_past,\n",
    "                attention_mask=attention_mask,\n",
    "                # head_mask=head_mask[i],\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                memory_state = memory_state, # Add\n",
    "                memory_state_u = memory_state_u\n",
    "            )\n",
    "    \n",
    "            hidden_states = outputs[0]\n",
    "            if use_cache is True:\n",
    "                presents = presents + (outputs[1],)\n",
    "    \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
    "    \n",
    "    \n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "    \n",
    "        hidden_states = hidden_states.view(output_shape)\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "    \n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "    \n",
    "        return memory_state_u, BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f5e78e07-7e43-4698-b5f9-cdabfbe08689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LMHeadModel(nn.Module):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.config = config\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.transformer.wte\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        memory_state = None, # Add\n",
    "        memory_state_u = None\n",
    "    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        memory_state_u, transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            memory_state = memory_state,\n",
    "            memory_state_u = memory_state_u,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return memory_state_u, CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b374ae-1e2c-4c81-b423-a743ec81e07d",
   "metadata": {},
   "source": [
    "#### Compare gpt2model with huggingface pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "eb43ffcf-d1c0-4a66-9225-92561f1b6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = GPT2Config()\n",
    "# config.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "23183a80-2364-4799-9a5c-eb01e4b7c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b598629d-a2ee-4b68-806f-26f4097eaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2LMHeadModel(config)\n",
    "# for name1, param1 in model.named_parameters():\n",
    "#     param1.requires_grad_(True)\n",
    "#     for name2, param2 in base_model.named_parameters():\n",
    "#         if name2 in name1:\n",
    "#             param1.data = param2.data.clone()\n",
    "# model.lm_head.weight.data = base_model.lm_head.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "682d6aba-30f8-45f3-8c19-0551c866f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = tokenizer('hello, world', return_tensors='pt')\n",
    "# model.eval()\n",
    "# # x = model(**input).logits\n",
    "# # x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5150a9e4-a742-4d9a-af28-0d59ed28a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input['encoder_hidden_states'] = torch.randn(1,5, 768)\n",
    "# input['encoder_attention_mask'] = torch.ones(1,5)\n",
    "# input['memory_state'] = torch.ones(1, 2, 768)\n",
    "# input['memory_state_u'] = torch.ones(1, 2, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5c225196-eef5-4e28-9838-1f17968697a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem, x = model(**input)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "f50b6931-b700-427e-961e-e017274a4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from itertools import chain\n",
    "from torch.utils.data import DataLoader#, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# sys.path.append('github')\n",
    "sys.path.append('..')\n",
    "device = 'cpu'\n",
    "\n",
    "input_size = 128\n",
    "memory_size = 2\n",
    "n_segments = 2\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "block_size = input_size\n",
    "block_size -= memory_size\n",
    "history_size = (n_segments - 1) * block_size\n",
    "\n",
    "def group_texts(examples, block_size, history_size=None):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if history_size is None:\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
    "    labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
    "    attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "    labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels,\n",
    "                'attention_mask': attention_mask}\n",
    "\n",
    "    if input_ids.shape[1] != block_size:\n",
    "        labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "        labels_mask[:, :-block_size] = False\n",
    "        collated['labels_mask'] = labels_mask\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "\n",
    "task_name = 'wikitext-2-v1'\n",
    "raw_datasets = datasets.load_dataset('wikitext', task_name)\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size),\n",
    "                                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
    "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size, history_size),\n",
    "                                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")\n",
    "\n",
    "\n",
    "train_rnd_generator = torch.Generator()\n",
    "train_rnd_generator.manual_seed(42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn,\n",
    "                                shuffle=True, drop_last=False, generator=train_rnd_generator, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                        collate_fn=collate_fn, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c73bb48b-dc46-430d-98e3-71479c97fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_dim =  getattr(self.model.config, 'n_embd', self.model.config.hidden_size)\n",
    "        memory_weights = torch.randn((num_mem_tokens, memory_dim)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        # self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        new_memory_state, out = self.model(**seg_kwargs)\n",
    "        out = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "\n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        cross_shape = torch.cat([memory_state, inputs_embeds], dim=1).shape\n",
    "        seg_kwargs['input_ids'] = input_ids\n",
    "        seg_kwargs['encoder_attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], cross_shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        seg_kwargs['attention_mask'] = kwargs['attention_mask']\n",
    "        seg_kwargs['encoder_hidden_states'] = torch.ones(cross_shape) # not necessary, only need this shape\n",
    "        seg_kwargs['memory_state'] = memory_state\n",
    "        seg_kwargs['memory_state_u'] = memory_state\n",
    "        return seg_kwargs\n",
    "\n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:] = attention_mask\n",
    "            return mask\n",
    "\n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            out['logits'] = model_outputs.logits\n",
    "\n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            out = model_outputs\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "import random\n",
    "class RecurrentWrapper(torch.nn.Module):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels,\n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions,\n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "\n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "        else:\n",
    "            out['loss'] = 0\n",
    "\n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        for seg_num, o in enumerate(cell_outputs):\n",
    "            for key, value in o.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return out\n",
    "\n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "\n",
    "        memory_state = memory_state.detach()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "302177ec-e498-4257-8ebb-151d7b43dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config()\n",
    "config.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "1866e724-8afc-4e5c-8c9b-09305fbbb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "a42d4396-f6e7-4bf5-b78d-06b29d046da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "for name1, param1 in model.named_parameters():\n",
    "    param1.requires_grad_(True)\n",
    "    for name2, param2 in base_model.named_parameters():\n",
    "        if name2 in name1:\n",
    "            param1.data = param2.data.clone()\n",
    "model.lm_head.weight.data = base_model.lm_head.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "0a3654d1-20dc-4b02-8c61-774bc3071804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentWrapper(\n",
       "  (memory_cell): MemoryCell(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (crossattention): GPT2Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (q_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (crossattention_u): GPT2Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (q_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (mlp_u): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_f_u): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = MemoryCell(model, num_mem_tokens=memory_size)\n",
    "model = RecurrentWrapper(cell,\n",
    "                        segment_size=block_size,\n",
    "                        max_n_segments=n_segments,\n",
    "                        )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "81cdb9df-6aa9-4b6d-ba3d-1f2896542f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optim = AdamW(params=model.parameters(), lr=1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "fb6f1a04-a07e-4171-b8e0-3385f8679559",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 200\n",
    "eval_steps = 100\n",
    "\n",
    "train_gen = iter(train_dataloader)\n",
    "valid_gen = iter(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4b20adc8-53a8-4b8e-ab94-9d56f7528a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f198c-64b9-46ff-b24e-88a388a4436c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebc0be623aa4b5da3f0349d35f08604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.121795654296875\n",
      "7.127286434173584\n",
      "5.5671234130859375\n",
      "4.7019429206848145\n",
      "5.061648368835449\n",
      "5.088944435119629\n",
      "4.501241207122803\n",
      "4.242065906524658\n",
      "4.482523441314697\n",
      "4.063199520111084\n",
      "4.374668121337891\n",
      "3.9162204265594482\n",
      "4.470005512237549\n",
      "4.507321834564209\n",
      "4.194787979125977\n",
      "4.620619773864746\n",
      "4.5959062576293945\n",
      "3.710200309753418\n",
      "4.349869728088379\n",
      "4.125412940979004\n",
      "4.646178722381592\n",
      "4.282745838165283\n",
      "3.7909562587738037\n",
      "4.572933197021484\n",
      "3.673898458480835\n",
      "4.3195672035217285\n",
      "3.9665958881378174\n",
      "3.4921865463256836\n",
      "3.5637149810791016\n",
      "3.8100435733795166\n",
      "4.006250858306885\n",
      "4.134266376495361\n",
      "3.5989882946014404\n",
      "3.938216209411621\n",
      "4.223481178283691\n",
      "3.797844886779785\n",
      "4.518538475036621\n",
      "3.4350502490997314\n",
      "3.6928889751434326\n",
      "3.669205665588379\n",
      "3.172811985015869\n",
      "4.427687168121338\n",
      "4.103123664855957\n",
      "3.930098056793213\n",
      "4.382216930389404\n",
      "3.995643377304077\n",
      "3.3819379806518555\n",
      "3.594146490097046\n",
      "3.5105767250061035\n",
      "3.7090535163879395\n",
      "4.1072306632995605\n",
      "4.159344673156738\n",
      "4.094944953918457\n",
      "3.5552761554718018\n",
      "3.688890218734741\n",
      "3.6676955223083496\n",
      "3.838357448577881\n",
      "3.54801344871521\n",
      "3.6263999938964844\n",
      "3.60520339012146\n",
      "3.6600770950317383\n",
      "3.818679094314575\n",
      "4.475074768066406\n",
      "3.924567937850952\n",
      "3.754655361175537\n",
      "3.6659820079803467\n",
      "3.8302087783813477\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for step in tqdm.notebook.tqdm(range(train_steps)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    batch = next(train_gen)\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "    out = model(**batch)\n",
    "    loss = out.loss\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # lr_scheduler.step()\n",
    "\n",
    "    losses.append(loss.detach().item())\n",
    "    print(loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82717b2e-12ed-403e-9fa4-5ec246dc6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2d9da-167a-4573-ac86-24fd836a956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_losses = []\n",
    "model.eval()\n",
    "for step in tqdm.notebook.tqdm(range(eval_steps)):\n",
    "    batch = next(valid_gen)\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "    valid_loss = out.loss\n",
    "\n",
    "    valid_losses.append(valid_loss.detach().item())\n",
    "    print(valid_loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7679a-aa9a-4fee-b1b7-0baaec7cf6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
